x <- as.matrix(data.frame(x0 = c(1,1,1),x1 = c(4,6,9)))
y <- as.matrix(data.frame(y=c(5,10,12)))
theta[2] <- t2_new
gradientDesc <- function(x, y, learn_rate, conv_threshold,max_iter) {
theta = matrix(0,2,1)
yhat <- theta[1]*x[1] + theta[2]*x[2]
MSE <- sum((y - yhat) ^ 2) / nrow(y)
converged = F
iterations = 0
while(converged == F) {
## Implement the gradient descent algorithm
t1_new <- theta[1] - learn_rate * ((2 / nrow(y)) * (sum((yhat-y) * x[1])))
t2_new <- theta[2] - learn_rate * ((2 / nrow(y)) * (sum((yhat-y) * x[2])))
theta[1] <- t1_new
theta[2] <- t2_new
yhat <- t1_new*x[1] + t2_new*x[2]
MSE_new <- sum((y - yhat) ^ 2) / nrow(y)
if(MSE - MSE_new <= conv_threshold) {
converged = T
return(paste("theta0:", t1_new, "theta1:", t2_new))
}
iterations = iterations + 1
if(iterations > max_iter) {
converged = T
return(paste("theta0:", t1_new, "theta1:", t2_new))
}
}
}
gradientDesc(x,y,0.01,0.000001,100000)
x <- as.matrix(c(4,6,9))
y <- as.matrix(c(5,10,12))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
View(model)
setwd("C:/Users/Juvenal/Desktop/SeminÃ¡rio RegressÃ£o/Linear Regression")
data <- read.csv('../teste.csv')
View(data)
attach(data)
plot(psi,weeks)
plot(weeks,psi)
x <- weeks
y <- psi
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
x <- as.matrix(weeks)
y <- as.matrix(psi)
x
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
linear_regression(x,y,10000,0.01)
x <- as.matrix(c(4,6,9,10))
y <- as.matrix(c(5,10,12,17))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
View(model)
x
x <- as.matrix(c(4,6,9,10,7,6,7))
y <- as.matrix(c(5,10,12,17,10,12,17))
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
View(model)
model <- lm(y~x)
View(model)
x <- as.matrix(c(4,6,9,10,7,6,7))
y <- as.matrix(c(5,10,12,17,10,12,17))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
View(model)
View(model)
x <- as.matrix(runif(50,10,100))
y <- as.matrix(runif(50,100,200))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
x <- as.matrix(runif(40,10,100))
y <- as.matrix(runif(40,100,200))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
x <- as.matrix(runif(10,10,100))
y <- as.matrix(runif(10,100,200))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
x <- as.matrix(runif(9,10,100))
y <- as.matrix(runif(9,100,200))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
x <- as.matrix(runif(8,10,100))
y <- as.matrix(runif(8,100,200))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
x <- as.matrix(runif(6,10,100))
y <- as.matrix(runif(6,100,200))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
x <- as.matrix(runif(4,10,100))
y <- as.matrix(runif(4,100,200))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
x <- as.matrix(runif(3,10,100))
y <- as.matrix(runif(3,100,200))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
x <- as.matrix(c(4,6,9,10,7,6,7))
y <- as.matrix(c(5,10,12,17,10,12,17))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
x <- as.matrix(c(4,6,9,10,7,6,7,9,10,12,20,21))
y <- as.matrix(c(5,10,12,17,10,12,17,20,19,18,16))
y <- as.matrix(c(5,10,12,17,10,12,17,20,19,18,16,20))
x <- as.matrix(c(4,6,9,10,7,6,7,9,10,12,20,21))
y <- as.matrix(c(5,10,12,17,10,12,17,20,19,18,16,20))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
plot(x,y)
x <- as.matrix(c(4,6,9,10,7,6,7,9,10,12,20))
y <- as.matrix(c(5,10,12,17,10,12,17,20,19,18,16))
linear_regression <- function(x, y, epochs, learning_rate){
m_current=0; b_current=0
N = nrow(y)
for(i in 1:epochs){
y_current = (m_current * x) + b_current
cost = sum((y - y_current)^2) / N
m_gradient = -(2/N) * sum(crossprod(x,(y - y_current)))
b_gradient = -(2/N) * sum(y - y_current)
m_current = m_current - (learning_rate * m_gradient)
b_current = b_current - (learning_rate * b_gradient)
}
return(list(m_current, b_current, cost))
}
linear_regression(x,y,10000,0.01)
model <- lm(y~x)
plot(x,y)
View(model)
View(model)
data <- read.csv('../teste.csv')
attach(data)
normalize(data)
?normalize
install.packages(normalize)
install.packages("bbmisc")
install.packages("BBmisc")
library(BBmisc)
normalize(data)
data.norm <- normalize(data)
data.norm <- normalize(data,"scale")
View(data.norm)
data.norm <- normalize(data,"standardize")
data.norm
data.norm <- normalize(data,"center")
data.norm
data.norm <- normalize(data,"range")
data.norm
data.norm <- (data - mean(data))/sd(data)
sd(data)
mean(data)
mean(data[,1])
data.norm <- (data[,1] - mean(data[,1]))/sd(data[,1])
data.norm
data.norm <- (data[,1] - mean(data[,1] + data[,2] + data[,3]))/sd(data[,1]+data[,2] + data[,3])
data.norm
data <- read.csv('../teste.csv')
attach(data)
View(data)
normalize(data)
View(data)
data <- read.csv('../teste.csv')
attach(data)
View(data)
View(data)
X <- as.matrix(c(1:5,size,bedrooms))
X
X <- as.matrix(c(1:5,size,bedrooms),5)
X
X <- as.matrix(c(1:5,size,bedrooms),3)
X
X <- as.matrix(1:5,size,bedrooms)
X
X <- matrix(c(1:5,size,bedrooms))
X
X <- matrix(c(rep(1,5),size,bedrooms))
X
X <- matrix(c(rep(1,5),size,bedrooms),3)
X
X <- matrix(c(rep(1,5),size,bedrooms),5)
X
Y <- price
grad <- function(x, y, theta) {
m <- nrow(y)
gradient <- (1/m)* (t(x) %*% ((x %*% t(theta)) - y))
return(t(gradient))
}
# define gradient descent update algorithm
grad.descent <- function(x, maxit){
theta <- matrix(0, nrow=1,ncol=ncol(x)) # Initialize the parameters
alpha = .01 # set learning rate
for (i in 1:maxit) {
theta <- theta - alpha  * grad(x, y, theta)
}
return(theta)
}
grad.descent(x,10000)
grad.descent(X,10000)
grad.descent <- function(x,y, maxit){
theta <- matrix(0, nrow=1,ncol=ncol(x)) # Initialize the parameters
alpha = .01 # set learning rate
for (i in 1:maxit) {
theta <- theta - alpha  * grad(x, y, theta)
}
return(theta)
}
grad.descent(X,Y,10000)
Y <- as.matrix(price)
grad.descent(X,Y,10000)
model <- lm(Y~X)
model
library(BBmisc)
data.norm <- normalize(data)
data <- read.csv('../teste.csv')
data <- read.csv('../teste.csv')
data.norm <- normalize(data)
attach(data.norm)
X <- matrix(c(rep(1,5),size,bedrooms),5)
Y <- as.matrix(price)
# define the gradient function dJ/dtheata: 1/m * (h(x)-y))*x where h(x) = x*theta
# in matrix form this is as follows:
grad <- function(x, y, theta) {
m <- nrow(y)
gradient <- (1/m)* (t(x) %*% ((x %*% t(theta)) - y))
return(t(gradient))
}
# define gradient descent update algorithm
grad.descent <- function(x,y, maxit){
theta <- matrix(0, nrow=1,ncol=ncol(x)) # Initialize the parameters
alpha = .01 # set learning rate
for (i in 1:maxit) {
theta <- theta - alpha  * grad(x, y, theta)
}
return(theta)
}
# results without feature scaling
print(grad.descent(X,Y,10000))
lm(Y~X)
3.626e-01
predict(model.predict(data[1:2])
)
predict(data[1:2],model)
?predict
plot(X,Y)
